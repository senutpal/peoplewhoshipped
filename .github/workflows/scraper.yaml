# =============================================================================
# Scheduled Leaderboard Data Scraper Workflow
# =============================================================================
#
# This workflow automates data collection from GitHub and Slack:
# 1. Prepares the PGlite database schema
# 2. Imports existing flat file data
# 3. Scrapes new GitHub activity (PRs, issues, commits, comments)
# 4. Scrapes new Slack EOD updates
# 5. Exports updated data to flat files
# 6. Uploads data as build artifacts
#
# Schedule: Runs every 12 hours automatically
# Manual: Can be triggered with custom "days" parameter
#
# =============================================================================

name: Scraper

on:
  # Automatic schedule - runs every 2 hours
  schedule:
    - cron: "0 */2 * * *"
  
  # Manual trigger with optional parameters
  workflow_dispatch:
    inputs:
      days:
        type: number
        description: "Number of days to scrape (how far back to look for activity)"
        default: 1

# Prevent multiple scraper runs from conflicting
concurrency:
  group: scheduled-data-scraper-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape-data:
    name: Scrape data
    runs-on: ubuntu-latest
    
    env:
      # =========================================================================
      # Database Configuration
      # =========================================================================
      # Path where PGlite will store the database files
      PGLITE_DB_PATH: ${{ github.workspace }}/data/pglite
      
      # Path to the data directory for flat file storage
      LEADERBOARD_DATA_PATH: ${{ github.workspace }}/data
      
      # Number of days to scrape (from workflow input or default)
      SCRAPE_DAYS: ${{ inputs.days || 1 }}
      
      # =========================================================================
      # GitHub Scraper Configuration
      # =========================================================================
      # GitHub token for API access (automatically provided by Actions)
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      # GitHub organization to scrape repositories from
      # Set via: Settings → Variables → Actions → GH_ORG
      GITHUB_ORG: ${{ vars.GH_ORG || github.repository_owner }}
      
      # =========================================================================
      # Slack Scraper Configuration
      # =========================================================================
      # Slack Bot User OAuth Token (xoxb-...)
      # Must have channels:history and users:read scopes
      SLACK_API_TOKEN: ${{ secrets.SLACK_API_TOKEN }}
      
      # Slack channel ID for EOD updates (e.g., C12345678)
      SLACK_CHANNEL: ${{ vars.SLACK_EOD_CHANNEL }}

    steps:
      # =========================================================================
      # Setup Steps
      # =========================================================================
      
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Shallow clone for faster checkout

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Install Dependencies
        run: bun install

      # =========================================================================
      # Database Preparation
      # =========================================================================
      
      - name: Prepare Database
        # Creates tables: contributors, activity_definitions, activities, slack_eod_queue
        run: bun run db:prepare

      - name: Import from Flat Data to Database
        # Imports existing data from data/ directory into PGlite
        run: bun run db:import

      # =========================================================================
      # Data Scraping
      # =========================================================================
      
      - name: Scrape GitHub Data
        # Scrapes: repositories, PRs, issues, comments, commits
        # Creates activities for: pr_opened, pr_merged, pr_reviewed, issue_opened, etc.
        run: bun run scrape:github

      - name: Scrape Slack Data
        # Scrapes: EOD messages from configured channel
        # Creates activities for: eod_update
        run: bun run scrape:slack

      # =========================================================================
      # Data Export & Build
      # =========================================================================
      
      - name: Export Data to Flat Files
        # Exports all activities and messages to JSON files in data/
        run: bun run db:export

      - name: Export Static JSON for Build
        # Generates static JSON files for Next.js build
        run: bun run db:prebuild-static

      # =========================================================================
      # Deploy to Vercel
      # =========================================================================
      # Vercel dashboard has Root Directory = apps/web
      # Run commands from repo root - Vercel uses .vercel config

      - name: Install Vercel CLI
        run: npm install -g vercel@latest

      - name: Link Vercel Project
        run: |
          mkdir -p .vercel
          cp apps/web/.vercel/project.json .vercel/project.json
        
      - name: Pull Vercel Environment
        run: vercel pull --yes --environment=production --token=${{ secrets.VERCEL_TOKEN }}

      - name: Build with Vercel
        run: vercel build --prod --token=${{ secrets.VERCEL_TOKEN }}
        env:
          PGLITE_DB_PATH: ${{ github.workspace }}/data/pglite
          LEADERBOARD_DATA_PATH: ${{ github.workspace }}/data

      - name: Deploy to Vercel
        run: vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_TOKEN }}

      # =========================================================================
      # Artifact Upload (for backup/debugging)
      # =========================================================================

      - name: Upload Scraped Data Artifact
        uses: actions/upload-artifact@v4
        with:
          name: leaderboard-data
          path: |
            data/github/
            data/slack/
            data/static/
          retention-days: 30


